# creating-my-own-visual-library-using-YOLO-CLIP-AI

**This project was made as an attempt to create my own visual library.**

I had used tools such as **YOLO**, **CLIP** and **speech_recognition** in an attempt to not only make a visual library that worked in tandem to YOLO, but also activate it via voice command

**NOTE**: the voice command is still currently under development

How each library was used:

**1. YOLO:**
YOLO v8 nano was the most simplest to implement, as it is a pretrained model that already draws bounding boxes on objects it is already trained to recognise, that being a total of 80 items.
However, I found that 80 items to be quite limiting and required many more items to be identified for future projects. As such, I had wanted to a create a visual library of my own.
Therefore, YOLO acted as my base/main recognition model, helping identifying objects it knows with relatively good accuracy, while adding my own visual library on top


**2. CLIP AI**
This is where it gets complicated.




